{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Práctica: similitud de documentos con bag-of-words y similitud del coseno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparamos los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un método para preprocesar el texto convirtiendo las palabras en minúsculas, eliminando los signos de puntuación, las stopwords, etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in /Users/luisguillen/.virtualenvs/PLN2/lib/python3.12/site-packages (1.1.2)\n",
      "Requirement already satisfied: nltk in /Users/luisguillen/.virtualenvs/PLN2/lib/python3.12/site-packages (3.9.1)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /Users/luisguillen/.virtualenvs/PLN2/lib/python3.12/site-packages (from python-docx) (5.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /Users/luisguillen/.virtualenvs/PLN2/lib/python3.12/site-packages (from python-docx) (4.9.0)\n",
      "Requirement already satisfied: click in /Users/luisguillen/.virtualenvs/PLN2/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/luisguillen/.virtualenvs/PLN2/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/luisguillen/.virtualenvs/PLN2/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/luisguillen/.virtualenvs/PLN2/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/luisguillen/.virtualenvs/PLN2/lib/python3.12/site-packages (from scikit-learn) (1.26.3)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.1-cp312-cp312-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp312-cp312-macosx_12_0_arm64.whl (11.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.1-cp312-cp312-macosx_14_0_arm64.whl (24.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.6.1 scipy-1.15.1 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/luisguillen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "from docx import Document\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "folder_path = './docs'  \n",
    "\n",
    "documents = []\n",
    "\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-záéíóúüñ\\s]', '', text)\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se leen los documentos .docx y se almacena el contenido\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.docx'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            doc = Document(file_path)\n",
    "            content = ''\n",
    "            for para in doc.paragraphs:\n",
    "                content += para.text + ' '\n",
    "            processed_content = preprocess_text(content)\n",
    "            documents.append(processed_content)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al leer el archivo {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un vocabulario global (bag-of-words) y se crea un índice de vocabulario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for doc in documents:\n",
    "    vocab.update(doc.split())\n",
    "    \n",
    "vocab = {word: idx for idx, word in enumerate(vocab)}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se representa cada documento como un vector en el espacio del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_vector(doc):\n",
    "    vector = np.zeros(vocab_size)\n",
    "    for word in doc.split():\n",
    "        if word in vocab:\n",
    "            vector[vocab[word]] += 1  # Aquí contamos la aparición de cada palabra\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la matriz de documentos y calculamos la similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de similitud del coseno:\n",
      "tensor([[1.0000, 0.0441, 0.0000, 0.0795, 0.0993, 0.1380, 0.0636, 0.0152, 0.0000,\n",
      "         0.0497, 0.1462, 0.0882],\n",
      "        [0.0441, 1.0000, 0.0953, 0.0000, 0.0000, 0.1027, 0.1243, 0.0339, 0.0000,\n",
      "         0.1109, 0.1883, 0.1094],\n",
      "        [0.0000, 0.0953, 1.0000, 0.0000, 0.0000, 0.0331, 0.1032, 0.0164, 0.0000,\n",
      "         0.0895, 0.0000, 0.0530],\n",
      "        [0.0795, 0.0000, 0.0000, 1.0000, 0.1001, 0.0741, 0.0577, 0.1286, 0.2166,\n",
      "         0.0000, 0.1360, 0.2488],\n",
      "        [0.0993, 0.0000, 0.0000, 0.1001, 1.0000, 0.0579, 0.0000, 0.1912, 0.2067,\n",
      "         0.0417, 0.1132, 0.0617],\n",
      "        [0.1380, 0.1027, 0.0331, 0.0741, 0.0579, 1.0000, 0.1297, 0.0354, 0.0174,\n",
      "         0.0772, 0.1310, 0.1028],\n",
      "        [0.0636, 0.1243, 0.1032, 0.0577, 0.0000, 0.1297, 1.0000, 0.0184, 0.0000,\n",
      "         0.0600, 0.1904, 0.1540],\n",
      "        [0.0152, 0.0339, 0.0164, 0.1286, 0.1912, 0.0354, 0.0184, 1.0000, 0.0862,\n",
      "         0.0191, 0.0909, 0.0339],\n",
      "        [0.0000, 0.0000, 0.0000, 0.2166, 0.2067, 0.0174, 0.0000, 0.0862, 1.0000,\n",
      "         0.0376, 0.1660, 0.3226],\n",
      "        [0.0497, 0.1109, 0.0895, 0.0000, 0.0417, 0.0772, 0.0600, 0.0191, 0.0376,\n",
      "         1.0000, 0.0708, 0.0863],\n",
      "        [0.1462, 0.1883, 0.0000, 0.1360, 0.1132, 0.1310, 0.1904, 0.0909, 0.1660,\n",
      "         0.0708, 1.0000, 0.3016],\n",
      "        [0.0882, 0.1094, 0.0530, 0.2488, 0.0617, 0.1028, 0.1540, 0.0339, 0.3226,\n",
      "         0.0863, 0.3016, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "doc_vectors = np.array([doc_to_vector(doc) for doc in documents])\n",
    "\n",
    "if doc_vectors.ndim == 1:\n",
    "    doc_vectors = doc_vectors.reshape(1, -1)\n",
    "\n",
    "doc_vectors = torch.tensor(doc_vectors, dtype=torch.float32)\n",
    "\n",
    "similarity_matrix = torch.nn.functional.cosine_similarity(doc_vectors.unsqueeze(1), doc_vectors.unsqueeze(0), dim=2)\n",
    "\n",
    "print(\"Matriz de similitud del coseno:\")\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se identifican qué documentos son más similares entre sí y cuáles son menos similares.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los documentos más similares son los documentos (8, 11) con una similitud de 0.3225609064102173\n",
      "Los documentos menos similares son los documentos (0, 2) con una similitud de 0.0\n"
     ]
    }
   ],
   "source": [
    "max_similarity = 0\n",
    "min_similarity = 1\n",
    "most_similar_pair = None\n",
    "least_similar_pair = None\n",
    "for i in range(len(documents)):\n",
    "    for j in range(i+1, len(documents)):\n",
    "        similarity = similarity_matrix[i, j]\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            most_similar_pair = (i, j)\n",
    "        if similarity < min_similarity:\n",
    "            min_similarity = similarity\n",
    "            least_similar_pair = (i, j)\n",
    "\n",
    "print(f\"Los documentos más similares son los documentos {most_similar_pair} con una similitud de {max_similarity}\")\n",
    "print(f\"Los documentos menos similares son los documentos {least_similar_pair} con una similitud de {min_similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se muestra la cantidad de palabras en el vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El vocabulario tiene 411 palabras\n"
     ]
    }
   ],
   "source": [
    "print(f\"El vocabulario tiene {vocab_size} palabras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último se calculan los vectores TF-IDF de los documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se usa TfidfVectorizer para convertir documentos a vectores TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "doc_vectors = vectorizer.fit_transform(documents).toarray()\n",
    "doc_vectors = torch.tensor(doc_vectors, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se calcula la matriz de similitud del coseno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de similitud del coseno con TF-IDF:\n",
      "tensor([[1.0000, 0.0223, 0.0000, 0.0557, 0.0752, 0.0899, 0.0200, 0.0135, 0.0000,\n",
      "         0.0241, 0.0736, 0.0416],\n",
      "        [0.0223, 1.0000, 0.0811, 0.0000, 0.0000, 0.0527, 0.0624, 0.0234, 0.0000,\n",
      "         0.0668, 0.1153, 0.0685],\n",
      "        [0.0000, 0.0811, 1.0000, 0.0000, 0.0000, 0.0221, 0.0793, 0.0146, 0.0000,\n",
      "         0.0618, 0.0000, 0.0376],\n",
      "        [0.0557, 0.0000, 0.0000, 1.0000, 0.0554, 0.0488, 0.0456, 0.0894, 0.1320,\n",
      "         0.0000, 0.0882, 0.1646],\n",
      "        [0.0752, 0.0000, 0.0000, 0.0554, 1.0000, 0.0363, 0.0000, 0.1230, 0.1276,\n",
      "         0.0275, 0.0771, 0.0348],\n",
      "        [0.0899, 0.0527, 0.0221, 0.0488, 0.0363, 1.0000, 0.0629, 0.0183, 0.0083,\n",
      "         0.0317, 0.0635, 0.0559],\n",
      "        [0.0200, 0.0624, 0.0793, 0.0456, 0.0000, 0.0629, 1.0000, 0.0085, 0.0000,\n",
      "         0.0212, 0.0895, 0.0811],\n",
      "        [0.0135, 0.0234, 0.0146, 0.0894, 0.1230, 0.0183, 0.0085, 1.0000, 0.0507,\n",
      "         0.0166, 0.0589, 0.0299],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1320, 0.1276, 0.0083, 0.0000, 0.0507, 1.0000,\n",
      "         0.0254, 0.1162, 0.2215],\n",
      "        [0.0241, 0.0668, 0.0618, 0.0000, 0.0275, 0.0317, 0.0212, 0.0166, 0.0254,\n",
      "         1.0000, 0.0258, 0.0403],\n",
      "        [0.0736, 0.1153, 0.0000, 0.0882, 0.0771, 0.0635, 0.0895, 0.0589, 0.1162,\n",
      "         0.0258, 1.0000, 0.1961],\n",
      "        [0.0416, 0.0685, 0.0376, 0.1646, 0.0348, 0.0559, 0.0811, 0.0299, 0.2215,\n",
      "         0.0403, 0.1961, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = torch.nn.functional.cosine_similarity(doc_vectors.unsqueeze(1), doc_vectors.unsqueeze(0), dim=2)\n",
    "\n",
    "print(\"Matriz de similitud del coseno con TF-IDF:\")\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se buscan los pares de documentos más y menos similares\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los documentos más similares son los documentos (8, 11) con una similitud de 0.2215\n",
      "Los documentos menos similares son los documentos (0, 2) con una similitud de 0.0000\n"
     ]
    }
   ],
   "source": [
    "max_similarity = float('-inf')\n",
    "min_similarity = float('inf')\n",
    "most_similar_pair = None\n",
    "least_similar_pair = None\n",
    "\n",
    "for i in range(len(documents)):\n",
    "    for j in range(i + 1, len(documents)):\n",
    "        similarity = similarity_matrix[i, j].item()  # Convertir tensor a número\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            most_similar_pair = (i, j)\n",
    "        if similarity < min_similarity:\n",
    "            min_similarity = similarity\n",
    "            least_similar_pair = (i, j)\n",
    "\n",
    "print(f\"Los documentos más similares son los documentos {most_similar_pair} con una similitud de {max_similarity:.4f}\")\n",
    "print(f\"Los documentos menos similares son los documentos {least_similar_pair} con una similitud de {min_similarity:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PLN2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
