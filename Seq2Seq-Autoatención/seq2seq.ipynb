{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCusJuMDkgT4"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# **Modelos del lenguaje basados en redes neuronales artificiales**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VZKZP-QkgT5"
      },
      "source": [
        "## **Modelos seq2seq 2**\n",
        "\n",
        "Cuando la entrada de cada paso del decodificador proviene de la salida del paso anterior, estamos hablando de un modelo Seq2Seq con realimentación (feedback). Esto es especialmente común en tareas como la generación de texto.\n",
        "\n",
        "1. **Inicio de la Secuencia**: Se inicia la generación con un token especial, como `<SOS>` (Start of Sequence).\n",
        "\n",
        "2. **Generación Paso a Paso**:\n",
        "   - En el primer paso, el decodificador recibe el `<SOS>` y el estado del codificador como entrada.\n",
        "   - El decodificador procesa esta entrada y genera una salida para este paso.\n",
        "   - La salida generada se convierte en la entrada para el siguiente paso, junto con el estado actualizado del decodificador.\n",
        "   - Este proceso se repite hasta que se genera un token especial de fin de secuencia (`<EOS>`, End of Sequence) o hasta alcanzar un límite máximo de longitud.\n",
        "\n",
        "3. **Ventajas y Desventajas**:\n",
        "   - **Ventajas**: Esta forma de generar secuencias puede ayudar a mantener la coherencia en las secuencias generadas, ya que cada nueva palabra o token tiene en cuenta lo que ya se ha generado.\n",
        "   - **Desventajas**: Puede ser más lento, ya que cada paso depende del anterior, y errores en un paso pueden propagarse y afectar los pasos siguientes.\n",
        "\n",
        "\n",
        "Imagina que tienes un modelo seq2seq entrenado para traducir inglés a español. Para la frase \"How are you?\", el proceso sería algo así:\n",
        "\n",
        "1. El codificador procesa \"How are you?\" y genera un vector latente.\n",
        "2. El decodificador recibe el vector latente y el token `<SOS>`.\n",
        "3. El decodificador genera \"¿Cómo\", actualiza su estado y toma \"¿Cómo\" como entrada para el siguiente paso.\n",
        "4. El decodificador genera \"estás\", actualiza su estado y toma \"estás\" como entrada para el siguiente paso.\n",
        "5. Y así sucesivamente, hasta generar `<EOS>` para indicar el final de la secuencia.\n",
        "\n",
        "Este modelo de generación permite que el decodificador tenga en cuenta no solo el contexto proporcionado por el codificador, sino también la estructura de la secuencia que está generando, paso a paso.\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"imgs/seq2seq_feedback.svg\" width=\"70%\">\n",
        "</p>\n",
        "\n",
        "\n",
        "### **Teacher Forcing**\n",
        "\n",
        "**Teacher forcing** es una técnica utilizada en el entrenamiento de modelos seq2seq en la que, en un porcentaje de las veces, se utiliza la salida real (etiqueta) de un paso de tiempo como entrada para el siguiente paso, en lugar de la salida predicha por el modelo. Esta técnica puede ayudar a acelerar la convergencia y mejorar el rendimiento del modelo, especialmente en las etapas iniciales del entrenamiento.\n",
        "\n",
        "#### **¿Cómo funciona?**\n",
        "\n",
        "1. **Durante el Entrenamiento**: En cada paso de tiempo y con una cierta probabilidad de que suceda, en lugar de pasar la predicción del modelo del paso anterior como entrada al siguiente paso, se pasa la palabra real de la secuencia objetivo. Esto proporciona al modelo información directa y clara sobre cómo debería haber respondido en el paso anterior, independientemente de si la predicción fue correcta o no.\n",
        "\n",
        "2. **Durante la Evaluación/Predicción**: El modelo debe generar secuencias por sí mismo, utilizando sus propias predicciones del paso anterior para el siguiente paso. Durante esta fase, no se utiliza \"teacher forcing\".\n",
        "\n",
        "#### **Ventajas de Teacher Forcing:**\n",
        "\n",
        "1. **Aprendizaje más Rápido**: Al proporcionar al modelo la respuesta correcta en cada paso, se reduce la propagación de errores a través de la secuencia, lo que puede llevar a un aprendizaje más rápido.\n",
        "\n",
        "2. **Mejor Rendimiento**: Puede resultar en un mejor rendimiento del modelo, especialmente en las primeras etapas del entrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evlRZmN2kgT6"
      },
      "source": [
        "### **Implementación traductor inglés a español**\n",
        "\n",
        "#### **Dataset Europarl**\n",
        "\n",
        "El conjunto de datos Europarl contiene las transcripciones de los procedimientos del Parlamento Europeo, proporcionando una valiosa fuente de textos paralelos en 21 idiomas europeos. Las oraciones están alineadas entre los idiomas, lo que lo hace especialmente útil para tareas de traducción automática.\n",
        "\n",
        "Descargamos el dataset Europarl para español-inglés. Una vez descargado tendremos dos ficheros de texto, uno para cada idioma con las frases alineadas. El código siguiente muestra las primeras frases de cada fichero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "cosVMsytkgT6",
        "outputId": "ce2a480f-99ac-4eba-f02e-5288e5fe6d43"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/europarl/europarl-v7.es-en.en'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-305f9959a8c8>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Leer el conjunto de datos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mingles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mespanol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_translation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchivo_ingles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchivo_espanol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Inglés:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mingles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Español:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mespanol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-305f9959a8c8>\u001b[0m in \u001b[0;36mread_translation\u001b[0;34m(archivo_ingles, archivo_espanol)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_translation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchivo_ingles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchivo_espanol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchivo_ingles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf_ingles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchivo_espanol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf_espanol\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0moracion_ingles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moracion_espanol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_ingles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_espanol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0moracion_ingles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moracion_espanol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/europarl/europarl-v7.es-en.en'"
          ]
        }
      ],
      "source": [
        "def read_translation(archivo_ingles, archivo_espanol):\n",
        "    with open(archivo_ingles, 'r', encoding='utf-8') as f_ingles, open(archivo_espanol, 'r', encoding='utf-8') as f_espanol:\n",
        "        for oracion_ingles, oracion_espanol in zip(f_ingles, f_espanol):\n",
        "            yield oracion_ingles.strip(), oracion_espanol.strip()\n",
        "\n",
        "\n",
        "archivo_ingles = 'data/europarl/europarl-v7.es-en.en'\n",
        "archivo_espanol = 'data/europarl/europarl-v7.es-en.es'\n",
        "\n",
        "# Leer el conjunto de datos\n",
        "for i, (ingles, espanol) in enumerate(read_translation(archivo_ingles, archivo_espanol)):\n",
        "    print('Inglés:', ingles)\n",
        "    print('Español:', espanol)\n",
        "    print('---')\n",
        "    if i == 15:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAMhWRnHl2wi",
        "outputId": "a8730a6d-24f1-401e-ada2-1b2ec05cf8cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.0.1 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.0.1\u001b[0m\u001b[31m\n",
            "\u001b[0mzsh:1: 2.0.0 not found\n"
          ]
        }
      ],
      "source": [
        "#! pip uninstall torch torchtext -y\n",
        "!pip install torch==2.0.1 torchtext==0.15.2\n",
        "!pip install portalocker>=2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dd-pfPVkgT7"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torchtext\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class Translation(Dataset):\n",
        "    def __init__(self, source_file, target_file):\n",
        "        self.ingles = []\n",
        "        self.espanol = []\n",
        "        self.tokenizer_es = get_tokenizer(\"spacy\", language=\"es_core_news_md\")\n",
        "        self.tokenizer_en = get_tokenizer(\"spacy\", language=\"en_core_web_md\")\n",
        "        self.vocab_es = torchtext.vocab.FastText(language='es', unk_init=torch.Tensor.normal_)  # <-- Mirar esto para ver si añadir el token <unk> al vocabulario\n",
        "        self.vocab_en = torchtext.vocab.FastText(language='en', unk_init=torch.Tensor.normal_)\n",
        "\n",
        "        self.vocab_en = self.add_sos_eos_unk_pad(self.vocab_en)\n",
        "        self.vocab_es = self.add_sos_eos_unk_pad(self.vocab_es)\n",
        "\n",
        "        self.archivo_ingles = source_file\n",
        "        self.archivo_espanol = target_file\n",
        "\n",
        "        # Leer el conjunto de datos\n",
        "        for ingles, espanol in self.read_translation():\n",
        "            self.ingles.append(ingles)\n",
        "            self.espanol.append(espanol)\n",
        "\n",
        "\n",
        "    def add_sos_eos_unk_pad(self, vocabulary):\n",
        "        words = vocabulary.itos\n",
        "        vocab = vocabulary.stoi\n",
        "        embedding_matrix = vocabulary.vectors\n",
        "\n",
        "        # Tokens especiales\n",
        "        sos_token = '<sos>'\n",
        "        eos_token = '<eos>'\n",
        "        pad_token = '<pad>'\n",
        "        unk_token = '<unk>'\n",
        "\n",
        "        # Inicializamos los vectores para los tokens especiales, por ejemplo, con ceros\n",
        "        sos_vector = torch.full((1, embedding_matrix.shape[1]), 1.)\n",
        "        eos_vector = torch.full((1, embedding_matrix.shape[1]), 2.)\n",
        "        pad_vector = torch.zeros((1, embedding_matrix.shape[1]))\n",
        "        unk_vector = torch.full((1, embedding_matrix.shape[1]), 3.)\n",
        "\n",
        "        # Añade los vectores al final de la matriz de embeddings\n",
        "        embedding_matrix = torch.cat((embedding_matrix, sos_vector, eos_vector, unk_vector, pad_vector), 0)\n",
        "\n",
        "        # Añade los tokens especiales al vocabulario\n",
        "        vocab[sos_token] = len(vocab)\n",
        "        vocab[eos_token] = len(vocab)\n",
        "        vocab[pad_token] = len(vocab)\n",
        "        vocab[unk_token] = len(vocab)\n",
        "\n",
        "        words.append(sos_token)\n",
        "        words.append(eos_token)\n",
        "        words.append(pad_token)\n",
        "        words.append(unk_token)\n",
        "\n",
        "        vocabulary.itos = words\n",
        "        vocabulary.stoi = vocab\n",
        "        vocabulary.vectors = embedding_matrix\n",
        "\n",
        "        default_stoi = defaultdict(lambda : len(vocabulary)-1, vocabulary.stoi)\n",
        "        vocabulary.stoi = default_stoi\n",
        "\n",
        "        return vocabulary\n",
        "\n",
        "\n",
        "    def read_translation(self):\n",
        "        with open(self.archivo_ingles, 'r', encoding='utf-8') as f_ingles, open(self.archivo_espanol, 'r', encoding='utf-8') as f_espanol:\n",
        "            for oracion_ingles, oracion_espanol in zip(f_ingles, f_espanol):\n",
        "                yield oracion_ingles.strip().lower(), oracion_espanol.strip().lower()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ingles)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.ingles[idx], self.espanol[idx]\n",
        "        tokens_ingles = self.tokenizer_en(item[0])\n",
        "        tokens_espanol = self.tokenizer_es(item[1])\n",
        "\n",
        "        tokens_ingles = tokens_ingles + ['<eos>']\n",
        "        tokens_espanol = ['<sos>'] + tokens_espanol + ['<eos>']\n",
        "\n",
        "        if not tokens_ingles or not tokens_espanol:\n",
        "            return torch.zeros(1, 300), torch.zeros(1, 300)\n",
        "            # raise RuntimeError(\"Una de las muestras está vacía.\")\n",
        "\n",
        "        tensor_ingles = self.vocab_en.get_vecs_by_tokens(tokens_ingles)\n",
        "        tensor_espanol = self.vocab_es.get_vecs_by_tokens(tokens_espanol)\n",
        "\n",
        "        indices_ingles = [self.vocab_en.stoi[token] for token in tokens_ingles] + [self.vocab_en.stoi['<pad>']]\n",
        "        indices_espanol = [self.vocab_es.stoi[token] for token in tokens_espanol] + [self.vocab_es.stoi['<pad>']]\n",
        "\n",
        "        return tensor_ingles, tensor_espanol, indices_ingles, indices_espanol\n",
        "\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    ingles_batch, espanol_batch, ingles_seqs, espanol_seqs = zip(*batch)\n",
        "    ingles_batch = pad_sequence(ingles_batch, batch_first=True, padding_value=0)\n",
        "    espanol_batch = pad_sequence(espanol_batch, batch_first=True, padding_value=0)\n",
        "\n",
        "    # Calcular la longitud máxima de la lista de listas de índices\n",
        "    pad = espanol_seqs[0][-1]  # token <pad>\n",
        "    max_len = max([len(l) for l in espanol_seqs])\n",
        "    for seq in espanol_seqs:\n",
        "        seq += [pad]*(max_len-len(seq))\n",
        "\n",
        "    return ingles_batch, espanol_batch, ingles_seqs, espanol_seqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-5Oswk5nA2l",
        "outputId": "95545ca8-03af-4f54-cdfb-a0b9d73b1c77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting es-core-news-md==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_md-3.7.0/es_core_news_md-3.7.0-py3-none-any.whl (42.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-md==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.1.2)\n",
            "Installing collected packages: es-core-news-md\n",
            "Successfully installed es-core-news-md-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-md==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.2)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download es_core_news_md\n",
        "!python -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4o4WCc0dkgT7",
        "outputId": "d633b204-8d27-44da-b1cc-f85248a9b3bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ".vector_cache/wiki.es.vec: 2.59GB [00:08, 305MB/s]                            \n",
            "  0%|          | 0/985667 [00:00<?, ?it/s]WARNING:torchtext.vocab.vectors:Skipping token b'985667' with 1-dimensional vector [b'300']; likely a header\n",
            "100%|██████████| 985667/985667 [01:50<00:00, 8913.58it/s]\n",
            ".vector_cache/wiki.en.vec: 6.60GB [01:19, 82.9MB/s]                            \n",
            "  0%|          | 0/2519370 [00:00<?, ?it/s]WARNING:torchtext.vocab.vectors:Skipping token b'2519370' with 1-dimensional vector [b'300']; likely a header\n",
            "100%|██████████| 2519370/2519370 [04:43<00:00, 8892.07it/s]\n"
          ]
        }
      ],
      "source": [
        "# archivo_ingles = 'data/europarl/europarl-v7.es-en.en'\n",
        "# archivo_espanol = 'data/europarl/europarl-v7.es-en.es'\n",
        "\n",
        "archivo_ingles = 'mock.en'\n",
        "archivo_espanol = 'mock.es'\n",
        "\n",
        "translation = Translation(archivo_ingles, archivo_espanol)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp7PTGGWkgT7"
      },
      "source": [
        "### **Modelo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXeB8c8ckgT7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIw2EcpTkgT8"
      },
      "source": [
        "##### **Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftl8YHArkgT8"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output, (hidden, cell) = self.rnn(x)\n",
        "        return output, (hidden, cell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04ouSomVuoxm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementación de la atención de Luong (tipo 'general').\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # Matriz de proyección para la variante \"general\"\n",
        "        self.W = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
        "\n",
        "    def forward(self, encoder_outputs, decoder_hidden):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            encoder_outputs (Tensor): Salidas del encoder,\n",
        "                dimensión (batch_size, seq_len, hidden_dim).\n",
        "            decoder_hidden (Tensor): Último estado oculto del decodificador\n",
        "                o estado actual, dimensión (batch_size, hidden_dim).\n",
        "\n",
        "        Returns:\n",
        "            context_vector (Tensor): Contexto resultante de la atención,\n",
        "                dimensión (batch_size, hidden_dim).\n",
        "            attn_weights (Tensor): Pesos de atención por cada paso temporal\n",
        "                de la secuencia de entrada, dimensión (batch_size, seq_len).\n",
        "        \"\"\"\n",
        "\n",
        "        # 1) Proyectamos el estado oculto del decodificador (batch_size, hidden_dim)\n",
        "        decoder_hidden_proj = self.W(decoder_hidden)  # (batch_size, hidden_dim)\n",
        "\n",
        "        # 2) Calculamos los puntajes de atención usando producto punto:\n",
        "        #    encoder_outputs:       (batch_size, seq_len, hidden_dim)\n",
        "        #    decoder_hidden_proj:   (batch_size, hidden_dim) -> (batch_size, hidden_dim, 1)\n",
        "        # => scores:               (batch_size, seq_len)\n",
        "        scores = torch.bmm(encoder_outputs, decoder_hidden_proj.unsqueeze(2)).squeeze(2)\n",
        "\n",
        "        # 3) Calculamos los pesos de atención aplicando softmax a los puntajes\n",
        "        attn_weights = F.softmax(scores, dim=1)  # (batch_size, seq_len)\n",
        "\n",
        "        # 4) Multiplicamos los pesos de atención por las salidas del encoder\n",
        "        #    para obtener el vector de contexto\n",
        "        #    attn_weights:   (batch_size, seq_len)\n",
        "        #    encoder_outputs:(batch_size, seq_len, hidden_dim)\n",
        "        # => context_vector:(batch_size, hidden_dim)\n",
        "        context_vector = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
        "\n",
        "        return context_vector, attn_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrxOILKMkgT8"
      },
      "source": [
        "##### **Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBKluTsIkgT8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Capa LSTM, recibe como entrada (input_dim + hidden_dim)\n",
        "        self.rnn = nn.LSTM(input_dim + hidden_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers,\n",
        "                           batch_first=True,\n",
        "                           dropout=dropout)\n",
        "\n",
        "        # Proyección final para obtener logits sobre el vocabulario\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        # Mecanismo de atención (Luong general)\n",
        "        self.attention = Attention(hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, hidden, cell, encoder_outputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): Embedding del token actual -> (batch_size, 1, input_dim)\n",
        "            hidden (Tensor): Estado oculto actual del LSTM -> (num_layers, batch_size, hidden_dim)\n",
        "            cell (Tensor): Estado de celda actual del LSTM -> (num_layers, batch_size, hidden_dim)\n",
        "            encoder_outputs (Tensor): Salidas del encoder -> (batch_size, seq_len, hidden_dim)\n",
        "\n",
        "        Returns:\n",
        "            output (Tensor): Logits proyectados -> (batch_size, 1, output_dim)\n",
        "            (hidden, cell) (tuple): Nuevos estados oculto y de celda del LSTM\n",
        "            attn_weights (Tensor): Pesos de atención -> (batch_size, seq_len)\n",
        "        \"\"\"\n",
        "        # 1) Extraemos el estado oculto de la última capa (para el mecanismo de atención)\n",
        "        dec_hidden = hidden[-1]  # (batch_size, hidden_dim)\n",
        "\n",
        "        # 2) Calculamos contexto y pesos de atención con la clase 'Attention'\n",
        "        context_vector, attn_weights = self.attention(encoder_outputs, dec_hidden)\n",
        "        context_vector = context_vector.unsqueeze(1)  # (batch_size, 1, hidden_dim)\n",
        "\n",
        "        # 3) Concatenar la entrada actual (x) con el contexto\n",
        "        rnn_input = torch.cat((x, context_vector), dim=2)  # (batch_size, 1, input_dim + hidden_dim)\n",
        "        rnn_input = self.dropout(rnn_input)\n",
        "\n",
        "        # 4) Pasar por la LSTM\n",
        "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
        "\n",
        "        # 5) Proyectamos para obtener la distribución de probabilidad sobre el vocab.\n",
        "        prediction = self.fc_out(output)  # (batch_size, 1, output_dim)\n",
        "\n",
        "        return prediction, (hidden, cell), attn_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoBY8Lkqs99M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.es_embeddings = torchtext.vocab.FastText(language='es')\n",
        "        self.M = self.es_embeddings.vectors\n",
        "        self.M = torch.cat((self.M, torch.zeros((4, self.M.shape[1]))), 0)\n",
        "\n",
        "    def forward(self, source, target, encoder_outputs=None, teacher_forcing_ratio=0.5):\n",
        "        target_len = target.shape[1]\n",
        "        batch_size = target.shape[0]\n",
        "\n",
        "        # Tensor para almacenar las salidas del decoder\n",
        "        outputs = torch.zeros(batch_size, target_len, 985671)\n",
        "\n",
        "        # Si encoder_outputs no está definido, procesa la fuente con el encoder\n",
        "        if encoder_outputs is None:\n",
        "            encoder_outputs, (hidden, cell) = self.encoder(source)\n",
        "        else:\n",
        "            # Si encoder_outputs está definido, no procesamos source nuevamente\n",
        "            hidden, cell = None, None  # O puedes inicializar con otros valores si es necesario\n",
        "\n",
        "        # La primera entrada al decoder es el vector <sos>\n",
        "        x = target[:, 0, :]\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            # Modification: Added '_' to ignore attention weights\n",
        "            output, (hidden, cell), _ = self.decoder(x.unsqueeze(1), hidden, cell, encoder_outputs)\n",
        "            outputs[:, t, :] = output.squeeze(1)\n",
        "\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            if teacher_force:\n",
        "                x = target[:, t, :]\n",
        "            else:\n",
        "                x = torch.matmul(output.squeeze(1), self.M)\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxrgG8DskgT8"
      },
      "outputs": [],
      "source": [
        "# Parámetros\n",
        "input_dim = 300\n",
        "output_dim = translation.vocab_es.vectors.shape[0]\n",
        "hidden_dim = 512\n",
        "num_layers = 2\n",
        "learning_rate = 0.001\n",
        "num_epochs = 30\n",
        "batch_size = 8\n",
        "num_workers = 0\n",
        "shuffle = True\n",
        "\n",
        "# Inicializa el modelo, el optimizador y la función de pérdida\n",
        "encoder = Encoder(input_dim, hidden_dim, num_layers)\n",
        "decoder = Decoder(input_dim, hidden_dim, output_dim, num_layers)\n",
        "model = Seq2Seq(encoder, decoder)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataloader = DataLoader(translation, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfkWnmYfkgT8",
        "outputId": "cb462640-19d6-4cd1-c6f2-7b46b2c172e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/30], Step [1/2], Loss: 41.4527\n",
            "Epoch [1/30], Average Loss: 41.3835\n",
            "Epoch [2/30], Step [1/2], Loss: 40.8925\n",
            "Epoch [2/30], Average Loss: 33.8316\n",
            "Epoch [3/30], Step [1/2], Loss: 39.0843\n",
            "Epoch [3/30], Average Loss: 38.0465\n",
            "Epoch [4/30], Step [1/2], Loss: 34.2481\n",
            "Epoch [4/30], Average Loss: 27.4220\n",
            "Epoch [5/30], Step [1/2], Loss: 27.6008\n",
            "Epoch [5/30], Average Loss: 26.6998\n",
            "Epoch [6/30], Step [1/2], Loss: 22.8337\n",
            "Epoch [6/30], Average Loss: 22.1145\n",
            "Epoch [7/30], Step [1/2], Loss: 18.9444\n",
            "Epoch [7/30], Average Loss: 19.5379\n",
            "Epoch [8/30], Step [1/2], Loss: 16.5222\n",
            "Epoch [8/30], Average Loss: 16.2344\n",
            "Epoch [9/30], Step [1/2], Loss: 14.0096\n",
            "Epoch [9/30], Average Loss: 14.5125\n",
            "Epoch [10/30], Step [1/2], Loss: 11.7373\n",
            "Epoch [10/30], Average Loss: 12.7249\n",
            "Epoch [11/30], Step [1/2], Loss: 10.1369\n",
            "Epoch [11/30], Average Loss: 10.5326\n",
            "Epoch [12/30], Step [1/2], Loss: 9.4058\n",
            "Epoch [12/30], Average Loss: 7.8522\n",
            "Epoch [13/30], Step [1/2], Loss: 7.7582\n",
            "Epoch [13/30], Average Loss: 7.7318\n",
            "Epoch [14/30], Step [1/2], Loss: 7.1766\n",
            "Epoch [14/30], Average Loss: 6.7517\n",
            "Epoch [15/30], Step [1/2], Loss: 6.5192\n",
            "Epoch [15/30], Average Loss: 5.8929\n",
            "Epoch [16/30], Step [1/2], Loss: 6.1059\n",
            "Epoch [16/30], Average Loss: 5.1663\n",
            "Epoch [17/30], Step [1/2], Loss: 5.9826\n",
            "Epoch [17/30], Average Loss: 5.7726\n",
            "Epoch [18/30], Step [1/2], Loss: 5.7698\n",
            "Epoch [18/30], Average Loss: 5.3308\n",
            "Epoch [19/30], Step [1/2], Loss: 5.3510\n",
            "Epoch [19/30], Average Loss: 5.3460\n",
            "Epoch [20/30], Step [1/2], Loss: 5.3650\n",
            "Epoch [20/30], Average Loss: 4.2464\n",
            "Epoch [21/30], Step [1/2], Loss: 4.9293\n",
            "Epoch [21/30], Average Loss: 4.9328\n",
            "Epoch [22/30], Step [1/2], Loss: 5.0382\n",
            "Epoch [22/30], Average Loss: 4.2413\n",
            "Epoch [23/30], Step [1/2], Loss: 4.9407\n",
            "Epoch [23/30], Average Loss: 4.2133\n",
            "Epoch [24/30], Step [1/2], Loss: 4.9127\n",
            "Epoch [24/30], Average Loss: 4.0936\n",
            "Epoch [25/30], Step [1/2], Loss: 4.7893\n",
            "Epoch [25/30], Average Loss: 4.0169\n",
            "Epoch [26/30], Step [1/2], Loss: 3.1378\n",
            "Epoch [26/30], Average Loss: 4.3332\n",
            "Epoch [27/30], Step [1/2], Loss: 4.7137\n",
            "Epoch [27/30], Average Loss: 4.9334\n",
            "Epoch [28/30], Step [1/2], Loss: 4.6422\n",
            "Epoch [28/30], Average Loss: 4.6971\n",
            "Epoch [29/30], Step [1/2], Loss: 4.6264\n",
            "Epoch [29/30], Average Loss: 3.8204\n",
            "Epoch [30/30], Step [1/2], Loss: 4.3405\n",
            "Epoch [30/30], Average Loss: 4.5451\n"
          ]
        }
      ],
      "source": [
        "# Bucle de entrenamiento\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, (src, tgt, src_indices, tgt_indices) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt)\n",
        "\n",
        "        tgt_indices = torch.tensor(tgt_indices, dtype=torch.long)\n",
        "        loss = 0\n",
        "        for t in range(1, tgt.shape[1]):\n",
        "            loss += criterion(output[:, t, :], tgt_indices[:, t])\n",
        "        # loss = criterion(output, torch.tensor(tgt_indices, dtype=torch.long))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 5 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {total_loss / len(dataloader):.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0RIapWwkgT9",
        "outputId": "2d7b4f38-5c69-4931-fef7-5dad02f7aaf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mi <eos>\n"
          ]
        }
      ],
      "source": [
        "# Test the model with input sentences\n",
        "model.eval()\n",
        "\n",
        "sentence = \"my cat\"\n",
        "\n",
        "# Convertir a vectores\n",
        "tokens = translation.tokenizer_en(sentence)\n",
        "tokens = tokens + ['<eos>']\n",
        "text_tensor = translation.vocab_en.get_vecs_by_tokens(tokens)\n",
        "text_tensor = text_tensor.unsqueeze(0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    encoder_outputs, (hidden, cell) = model.encoder(text_tensor)\n",
        "\n",
        "outputs = []\n",
        "\n",
        "input_token = torch.tensor(translation.vocab_es.stoi['<sos>']).unsqueeze(0)\n",
        "input_token = translation.vocab_es.vectors[input_token].unsqueeze(0)\n",
        "\n",
        "\n",
        "for _ in range(5):\n",
        "    with torch.no_grad():\n",
        "        # Modification: Added '_' to ignore attention weights\n",
        "        output, (hidden, cell), _ = model.decoder(input_token, hidden, cell, encoder_outputs) # teacher_forcing_ratio=0.0\n",
        "\n",
        "    # Obtener el token con la probabilidad más alta\n",
        "    best_guess = output.argmax(2).squeeze(0)\n",
        "    outputs.append(best_guess.item())\n",
        "\n",
        "    # Si el token es <eos>, terminar la traducción\n",
        "    if best_guess == translation.vocab_es.stoi['<eos>']:\n",
        "        break\n",
        "\n",
        "    # Utilizar la palabra predicha como la siguiente entrada al decoder\n",
        "    input_token = translation.vocab_es.vectors[best_guess].unsqueeze(0)\n",
        "\n",
        "# Convertir los índices de salida a palabras\n",
        "translated_sentence = [translation.vocab_es.itos[idx] for idx in outputs]\n",
        "\n",
        "result = ' '.join(translated_sentence)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFbagMdkkgT9"
      },
      "outputs": [],
      "source": [
        "# Guardar el modelo\n",
        "torch.save(model.state_dict(), 'seq2seq.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZUJRz4HkgT9",
        "outputId": "2648dc5d-5bb2-4091-c3ba-6a9d5ead4dc2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load model from file\n",
        "model.load_state_dict(torch.load('seq2seq.pth'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PLN2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
