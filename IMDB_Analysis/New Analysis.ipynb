{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<img src=\"../imgs/EII-ULPGC-logo.jpeg\" width=\"430px\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NOTEBOOK 9**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Estrategias de clasificación**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Caso de uso: análisis de sentimientos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El *sentiment analysis* o análisis de sentimientos utiliza técnicas de procesamiento de lenguaje natural para identificar y extraer información subjetiva de un texto, buscando determinar el estado de ánimo o emoción expresada. Puede identificar polaridad (positivo, negativo, neutral), intensidad del sentimiento y sentimientos específicos hacia aspectos concretos. Se emplea en monitoreo de redes sociales, análisis de reseñas de productos, servicio al cliente y análisis de tendencias. A pesar de su utilidad, enfrenta desafíos como ironías, sarcasmos y contextos culturales que complican su precisión.\n",
    "\n",
    "\n",
    "El dataset de análisis de sentimientos de **IMDB (Internet Movie Database)** creado por Stanford es uno de los datasets más populares y ampliamente utilizados para la clasificación de sentimientos en análisis de texto. Fue creado por investigadores de la Universidad de Stanford y se utiliza principalmente para la tarea de clasificar las revisiones de películas en \"positivas\" o \"negativas\".\n",
    "\n",
    "Características principales del dataset:\n",
    "\n",
    "1. **Tamaño**: Contiene 50,000 reseñas, de las cuales 25,000 son etiquetadas como positivas y 25,000 como negativas.\n",
    "2. **Balanceado**: Es un dataset equilibrado, lo que significa que hay un número igual de reseñas positivas y negativas.\n",
    "3. **Contenido**: Las reseñas provienen de IMDB, y son reseñas de películas escritas por usuarios. \n",
    "4. **Etiquetado**: Las revisiones etiquetadas como positivas tienen una puntuación ≥ 7 sobre 10, mientras que las etiquetadas como negativas tienen una puntuación ≤ 4 sobre 10. Las reseñas con puntuaciones intermedias no están incluidas en el conjunto de datos.\n",
    "\n",
    "Este dataset es especialmente útil para entrenar modelos de machine learning para análisis de sentimiento y ha sido utilizado en varios trabajos de investigación y proyectos. Además, dada su popularidad, muchos frameworks y bibliotecas, como TensorFlow y PyTorch, a menudo proporcionan utilidades para cargar fácilmente este conjunto de datos para experimentación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el fichero IMDB\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('IMDB_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Phil the Alien is one of those quirky films wh...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I saw this movie when I was about 12 when it c...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>So im not a big fan of Boll's work but then ag...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The cast played Shakespeare.&lt;br /&gt;&lt;br /&gt;Shakes...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>This a fantastic movie of three prisoners who ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review sentiment\n",
       "0   One of the other reviewers has mentioned that ...  positive\n",
       "1   A wonderful little production. <br /><br />The...  positive\n",
       "2   I thought this was a wonderful way to spend ti...  positive\n",
       "3   Basically there's a family where a little boy ...  negative\n",
       "4   Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5   Probably my all-time favorite movie, a story o...  positive\n",
       "6   I sure would like to see a resurrection of a u...  positive\n",
       "7   This show was an amazing, fresh & innovative i...  negative\n",
       "8   Encouraged by the positive comments about this...  negative\n",
       "9   If you like original gut wrenching laughter yo...  positive\n",
       "10  Phil the Alien is one of those quirky films wh...  negative\n",
       "11  I saw this movie when I was about 12 when it c...  negative\n",
       "12  So im not a big fan of Boll's work but then ag...  negative\n",
       "13  The cast played Shakespeare.<br /><br />Shakes...  negative\n",
       "14  This a fantastic movie of three prisoners who ...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a desarrollar un pipeline usando `scikit-learn` para la clasificación de sentimientos con el dataset de IMDB. El proceso general que vamos a seguir es:\n",
    "\n",
    "1. Cargar los datos.\n",
    "2. Preprocesar los datos.\n",
    "3. Dividir los datos en conjuntos de entrenamiento y prueba.\n",
    "4. Crear un clasificador Naive Bayes.\n",
    "5. Entrenar el clasificador y comprobar los resultados con los datos de prueba.\n",
    "\n",
    "Vamos a ello:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8472\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.86      0.85      6157\n",
      "    positive       0.86      0.84      0.85      6343\n",
      "\n",
      "    accuracy                           0.85     12500\n",
      "   macro avg       0.85      0.85      0.85     12500\n",
      "weighted avg       0.85      0.85      0.85     12500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#quita los warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Importando las bibliotecas necesarias\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Dividimos el DataFrame en datos y etiquetas\n",
    "X = df['review']\n",
    "y = df['sentiment']\n",
    "\n",
    "# Dividimos los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Creamos un vectorizador y un clasificador Naive Bayes\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=10000)\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Creamos un pipeline que incluya el vectorizador y el clasificador\n",
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('clf', clf)\n",
    "])\n",
    "\n",
    "# Entrenamos el modelo\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Comprobar los resultados con los datos de prueba\n",
    "predictions = pipeline.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Interpretación de las medidas de desempeño de un modelo de clasificación**\n",
    "\n",
    "Las medidas de desempeño son esenciales para evaluar la calidad y eficacia de un modelo de clasificación. Vamos a explorar las medidas de desempeño más comunes para modelos de clasificación binaria. Supongamos que nuestro dataset de test está compuesto por 8 muestras positivas (azules) y 6 negativas (rojas). \n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"imgs/metrics1.svg\" width=\"27%\">\n",
    "</div>\n",
    "\n",
    "Una vez que hemos entrenado nuestro modelo, lo aplicamos sobre el conjunto de test y obtenemos las siguientes predicciones:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"imgs/metrics2.svg\" width=\"27%\">\n",
    "</div>\n",
    "\n",
    "Éste determina que hay 7 muestras positivas y el resto negativas. Por tanto, el clasificador ha cometido errores. En realidad, tenemos 5 verdaderos positivos, 2 falsos positivos, 4 verdaderos negativos y 3 falsos negativos. Las medidas que tenemos para evaluar el desempeño del modelo son las siguientes:\n",
    "\n",
    "1. **Accuracy (Exactitud)**:\n",
    "Es la proporción de predicciones correctas entre el número total de casos. Se calcula como el número de verdaderos positivos más el número de verdaderos negativos dividido por el total de casos.\n",
    "    \n",
    "$$ \\text{Accuracy} = \\frac{\\text{Verdaderos Positivos + Verdaderos Negativos}}{\\text{Total de Casos}} $$\n",
    "\n",
    "2. **Precision (Precisión)**:\n",
    "Es la proporción de verdaderos positivos entre el número total de positivos predichos por el modelo.\n",
    "    \n",
    "$$ \\text{Precision} = \\frac{\\text{Verdaderos Positivos}}{\\text{Verdaderos Positivos + Falsos Positivos}} $$\n",
    "\n",
    "3. **Recall (Sensibilidad o Tasa de Verdaderos Positivos)**:\n",
    "Es la proporción de verdaderos positivos entre el número total de casos reales positivos.\n",
    "\n",
    "$$ \\text{Recall} = \\frac{\\text{Verdaderos Positivos}}{\\text{Verdaderos Positivos + Falsos Negativos}} $$\n",
    "\n",
    "4. **F1-Score (Puntuación F1)**:\n",
    "Es una métrica que combina tanto la precisión como el recall en una sola cifra. Es especialmente útil si las clases están desequilibradas. El valor de F1-Score varía entre 0 y 1, donde 1 indica un rendimiento perfecto y 0 indica un rendimiento pobre.\n",
    "\n",
    "$$ \\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$\n",
    "\n",
    "Observa que el F1-Score es la **media armónica** de la precisión y el recall. La media armónica es una media ponderada que da más peso a los valores más pequeños. Por tanto, el F1-Score es más sensible a los valores pequeños de precisión y recall que la media aritmética. \n",
    "\n",
    "5. **Support (Soporte)**:\n",
    "Refiere al número de muestras reales que se encuentran en una determinada clase. Es esencialmente el número de observaciones de la clase verdadera, sin tener en cuenta las predicciones del modelo.\n",
    "\n",
    "Cada una de estas métricas ofrece una perspectiva diferente sobre la eficacia del modelo y, en conjunto, proporcionan una imagen completa del rendimiento del modelo en diversas situaciones. Por ejemplo, en casos donde el costo de un falso positivo es alto, se podría priorizar la precisión, mientras que en situaciones donde el costo de un falso negativo es alto, se podría priorizar el recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Medias aritméticas vs. medias geométricas vs. medias armónicas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media aritmética: 0.5\n",
      "Media geométrica: 0.3556893304490063\n",
      "Media armónica: 0.2288135593220339\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#   MEDIA ARITMÉTICA, GEOMÉTRICA Y ARMÓNICA\n",
    "#\n",
    "\n",
    "a=0.1\n",
    "b=0.5\n",
    "c=0.9\n",
    "\n",
    "\n",
    "print(\"Media aritmética: \" + str((a+b+c)/3))\n",
    "print(\"Media geométrica: \" + str((a*b*c)**(1/3)))\n",
    "print(\"Media armónica: \" + str(3/(1/a+1/b+1/c)))\n",
    "\n",
    "# Observa que, a menos que los tres números sean iguales, la media geométrica es siempre menor que la media aritmética, y la media armónica es siempre menor que la media geométrica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Ejercicio 1\n",
    "\n",
    "¿Cómo se interpretan estas métricas cuanto la classificación es multiclase?\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Accuracy en multiclase:\n",
    "La accuracy en clasificación multiclase se calcula de la misma manera que en la clasificación binaria. Es la proporción de predicciones correctas (donde la clase predicha es igual a la clase real) entre el número total de ejemplos. Se sigue usando la misma fórmula:\n",
    "$$ \\text{Accuracy} = \\frac{\\text{Verdaderos Positivos + Verdaderos Negativos}}{\\text{Total de Casos}} $$\n",
    "\n",
    "### 2) Precision, Recall y F1-Score en multiclase:\n",
    "Estas métricas son más complicadas en el contexto multiclase porque ahora hay más de dos clases. Para calcularlas, se suelen aplicar dos enfoques principales: macro y micro promediado, además del enfoque ponderado.\n",
    "\n",
    "##### a) Micro promedio:\n",
    "El enfoque micro considera cada predicción como una decisión individual sin importar la clase. Suma los verdaderos positivos, falsos positivos, y falsos negativos de todas las clases y luego calcula las métricas:\n",
    "    \n",
    "-> Precision micro: Calcula la precisión global como si todas las clases fueran una sola clase.\n",
    "    $$ \\text{Precision micro} = \\frac{{∑Verdaderos Positivos}}{{∑(Verdaderos Positivos+Falsos Positivos)}} $$\n",
    "-> Recall micro: Calcula la tasa de verdaderos positivos para todas las clases combinadas.\n",
    "    $$ \\text{Recall Micro} = \\frac{{∑Verdaderos Positivos}}{{∑(Verdaderos Positivos+Falsos Negativos)}} $$\n",
    "-> F1-Score micro: Usa la precisión y el recall calculados de manera micro para obtener el F1-Score.\n",
    "    $$ \\text{F1 Micro} = \\frac{{2 × PrecisionMicro × RecallMicro}}{{Precision Micro + Recall Micro}} $$\n",
    "\n",
    "#### b) Macro promedio:\n",
    "El enfoque macro calcula las métricas para cada clase individualmente y luego promedia los resultados. Esto le da el mismo peso a cada clase, sin importar cuántas observaciones tenga cada una.\n",
    "\n",
    "-> Precision macro: Se calcula la precisión para cada clase y luego se promedia.\n",
    "    $$ \\text{Precision Macro} = \\frac{{1}}{{n}}\\sum_{i=1}^{n} \\text{Precision}_i $$\n",
    "-> Recall macro: Se calcula el recall para cada clase y luego se promedia.\n",
    "    $$ \\text{Recall Macro} = \\frac{{1}}{{n}}\\sum_{i=1}^{n} \\text{Recall}_i $$\n",
    "-> F1-Score macro: Calcula el F1-Score para cada clase y luego se promedia.\n",
    "    $$ \\text{F1 Macro} = \\frac{{1}}{{n}}\\sum_{i=1}^{n} \\text{F1}_i $$\n",
    "\n",
    "#### c) Ponderado:\n",
    "El enfoque ponderado tiene en cuenta el número de observaciones en cada clase. Calcula las métricas por clase y luego las promedia ponderando por el tamaño de cada clase.\n",
    "\n",
    "-> Precision ponderada:\n",
    "    $$ \\text{Precisión Ponderada} = \\frac{{1}}{{N}}\\sum_{i=1}^{n} \\text{Precisión}_i × \\text{Support}_i $$\n",
    "(Donde $\\text{Support}_i$ es el número de muestras reales en la clase 𝑖 y 𝑁 es el total de muestras.)\n",
    "-> Recall ponderado y F1 ponderado se calculan de manera similar.\n",
    "\n",
    "### 3) Support (Soporte):\n",
    "En el contexto multiclase, el soporte sigue siendo el número de muestras reales en cada clase. Esto es útil para entender la distribución de las clases y puede ayudar a interpretar si algunas clases están poco representadas, lo que puede afectar las métricas ponderadas.\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Ejercicio 2\n",
    "\n",
    "Realiza la clasificación del dataset \"20 Newsgroups\". Para ello, usa el clasificador Naive Bayes y Regresión Logística. Compara los resultados obtenidos. Busca la configuración óptima de parámetros para cada clasificador. Intenta mejorar los resultados obtenidos con otros clasificadores que puedas encontrar en la librería Scikit-Learn. Reporta la mejor solución de todas.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero cargamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "newsgroups_data = fetch_20newsgroups(subset='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego dividimos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    newsgroups_data.data,\n",
    "    newsgroups_data.target,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=newsgroups_data.target\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se un pipeline que incluya el vectorizador y el clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words='english', max_features=10000)),\n",
    "    ('clf', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último entrenamos el modelo y lo evaluamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8448275862068966\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.93      0.89       160\n",
      "           1       0.67      0.82      0.73       195\n",
      "           2       0.86      0.03      0.06       197\n",
      "           3       0.58      0.82      0.68       196\n",
      "           4       0.77      0.91      0.83       193\n",
      "           5       0.78      0.88      0.83       198\n",
      "           6       0.82      0.84      0.83       195\n",
      "           7       0.85      0.90      0.88       198\n",
      "           8       0.86      0.94      0.90       199\n",
      "           9       0.94      0.94      0.94       199\n",
      "          10       0.98      0.94      0.96       200\n",
      "          11       0.95      0.94      0.95       198\n",
      "          12       0.82      0.86      0.84       197\n",
      "          13       0.95      0.88      0.91       198\n",
      "          14       0.95      0.90      0.93       197\n",
      "          15       0.94      0.94      0.94       199\n",
      "          16       0.84      0.92      0.88       182\n",
      "          17       0.98      0.92      0.95       188\n",
      "          18       0.87      0.85      0.86       155\n",
      "          19       0.86      0.69      0.77       126\n",
      "\n",
      "    accuracy                           0.84      3770\n",
      "   macro avg       0.86      0.84      0.83      3770\n",
      "weighted avg       0.86      0.84      0.83      3770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "predictions = pipeline.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora probaremos a utilizar diferentes clasificadores para probar que tan buenos son "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline_log = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words='english', max_features=10000)),\n",
    "    ('clf', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8931034482758621\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92       160\n",
      "           1       0.78      0.84      0.81       195\n",
      "           2       0.84      0.82      0.83       197\n",
      "           3       0.77      0.79      0.78       196\n",
      "           4       0.88      0.84      0.86       193\n",
      "           5       0.85      0.90      0.88       198\n",
      "           6       0.83      0.86      0.84       195\n",
      "           7       0.92      0.90      0.91       198\n",
      "           8       0.95      0.91      0.93       199\n",
      "           9       0.96      0.96      0.96       199\n",
      "          10       0.97      0.97      0.97       200\n",
      "          11       0.96      0.95      0.96       198\n",
      "          12       0.82      0.83      0.83       197\n",
      "          13       0.89      0.94      0.92       198\n",
      "          14       0.98      0.94      0.96       197\n",
      "          15       0.91      0.94      0.93       199\n",
      "          16       0.91      0.91      0.91       182\n",
      "          17       0.97      0.94      0.95       188\n",
      "          18       0.89      0.88      0.89       155\n",
      "          19       0.91      0.76      0.83       126\n",
      "\n",
      "    accuracy                           0.89      3770\n",
      "   macro avg       0.89      0.89      0.89      3770\n",
      "weighted avg       0.89      0.89      0.89      3770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_log.fit(X_train, y_train)\n",
    "\n",
    "predictions_log = pipeline_log.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions_log))\n",
    "print(classification_report(y_test, predictions_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "pipeline_SVC = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words='english', max_features=10000)),\n",
    "    ('clf', SVC(kernel='linear'))\n",
    "])\n",
    "\n",
    "pipeline_LinSVC = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words='english', max_features=10000)),\n",
    "    ('clf', LinearSVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8591511936339522\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.91      0.89       160\n",
      "           1       0.70      0.81      0.75       195\n",
      "           2       0.84      0.79      0.81       197\n",
      "           3       0.71      0.77      0.74       196\n",
      "           4       0.84      0.76      0.80       193\n",
      "           5       0.86      0.86      0.86       198\n",
      "           6       0.81      0.83      0.82       195\n",
      "           7       0.88      0.90      0.89       198\n",
      "           8       0.93      0.90      0.91       199\n",
      "           9       0.91      0.90      0.91       199\n",
      "          10       0.92      0.94      0.93       200\n",
      "          11       0.94      0.93      0.94       198\n",
      "          12       0.77      0.79      0.78       197\n",
      "          13       0.84      0.85      0.84       198\n",
      "          14       0.99      0.91      0.95       197\n",
      "          15       0.88      0.91      0.89       199\n",
      "          16       0.89      0.87      0.88       182\n",
      "          17       0.96      0.90      0.93       188\n",
      "          18       0.85      0.88      0.86       155\n",
      "          19       0.84      0.73      0.78       126\n",
      "\n",
      "    accuracy                           0.86      3770\n",
      "   macro avg       0.86      0.86      0.86      3770\n",
      "weighted avg       0.86      0.86      0.86      3770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_SVC.fit(X_train, y_train)\n",
    "\n",
    "predictions_SVC = pipeline_SVC.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions_SVC))\n",
    "print(classification_report(y_test, predictions_SVC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8824933687002653\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       160\n",
      "           1       0.77      0.84      0.80       195\n",
      "           2       0.81      0.84      0.82       197\n",
      "           3       0.74      0.75      0.74       196\n",
      "           4       0.84      0.83      0.83       193\n",
      "           5       0.88      0.87      0.88       198\n",
      "           6       0.80      0.82      0.81       195\n",
      "           7       0.91      0.90      0.91       198\n",
      "           8       0.96      0.91      0.94       199\n",
      "           9       0.95      0.93      0.94       199\n",
      "          10       0.97      0.98      0.98       200\n",
      "          11       0.95      0.96      0.95       198\n",
      "          12       0.82      0.81      0.81       197\n",
      "          13       0.86      0.92      0.89       198\n",
      "          14       0.98      0.93      0.96       197\n",
      "          15       0.91      0.93      0.92       199\n",
      "          16       0.90      0.90      0.90       182\n",
      "          17       0.97      0.93      0.95       188\n",
      "          18       0.90      0.88      0.89       155\n",
      "          19       0.86      0.75      0.80       126\n",
      "\n",
      "    accuracy                           0.88      3770\n",
      "   macro avg       0.88      0.88      0.88      3770\n",
      "weighted avg       0.88      0.88      0.88      3770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_LinSVC.fit(X_train, y_train)\n",
    "\n",
    "predictions_LinSVC = pipeline_LinSVC.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions_LinSVC))\n",
    "print(classification_report(y_test, predictions_LinSVC))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PLN2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
